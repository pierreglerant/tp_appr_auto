{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "charpy_columns = [\"Charpy impact toughness (J)\"]\n",
    "stress_columns = [\"Yield strength (MPa)\", \"Ultimate tensile strength (MPa)\", \"Elongation (%)\", \"Reduction of Area (%)\"]\n",
    "target_columns = charpy_columns + stress_columns\n",
    "\n",
    "data = pd.read_csv('../../data/data_no_reg_no_dup.csv')\n",
    "X = data.drop(target_columns,axis=1)\n",
    "y = data[target_columns]\n",
    "\n",
    "def predict_confidence(model, X):\n",
    "    return 1 / (1 + np.abs(model.predict(X)))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, size_vector):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(size_vector)-1):\n",
    "            self.layers.append(nn.Linear(size_vector[i],size_vector[i+1]))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.layers.pop(-1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "target_columns_bis = [\"Charpy impact toughness (J)\",\"Yield strength (MPa)\", \"Ultimate tensile strength (MPa)\"]\n",
    "for target_column in target_columns_bis:\n",
    "\n",
    "    print(\"=========================================================================================\\n\")\n",
    "    print(f\"====================={target_column}========================\\n\")\n",
    "    print(\"=========================================================================================\\n\")\n",
    "    X_supervised = X[y[target_column].notna()]\n",
    "    y_supervised = y[y[target_column].notna()]\n",
    "\n",
    "    X_unlabeled = np.array(X[y[target_column].isna()])\n",
    "    y_unlabeled = np.array(y.loc[y[target_column].isna(),target_column])\n",
    "\n",
    "    X_tensor = torch.tensor(X_supervised.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_supervised[target_column].values, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X_tensor,y_tensor)\n",
    "\n",
    "    train_size = int(0.6 * len(dataset))  # 70% for training\n",
    "    val_size = int(0.2 * len(dataset))    # 15% for validation\n",
    "    test_size = len(dataset) - train_size - val_size  # Remaining for test\n",
    "\n",
    "    # Step 2: Split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    training_data = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    for data, target in training_data:\n",
    "        data_list.append(data.numpy())\n",
    "        target_list.append(target.numpy())\n",
    "\n",
    "    X_training = np.array(data_list)\n",
    "    y_training = np.array(target_list)\n",
    "\n",
    "    print(\"==================================KNN==================================\\n\")\n",
    "\n",
    "    neib_vect = [1,2,3,4,5,6,7,8,9,10,15,20,25,30]\n",
    "    mean_error = []\n",
    "    var_error = []\n",
    "    train_mse = []\n",
    "    for n_neib in tqdm(neib_vect):\n",
    "        current_errors = []\n",
    "        for i in range(50):\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1415)\n",
    "            Xscaler = StandardScaler()\n",
    "            yscaler = StandardScaler()\n",
    "            X_train = Xscaler.fit_transform(X_train)\n",
    "            X_val = Xscaler.transform(X_val)\n",
    "            y_train = yscaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
    "            y_val = yscaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "            \n",
    "            knn = KNeighborsRegressor(n_neighbors=n_neib)\n",
    "\n",
    "            knn.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = knn.predict(X_val)\n",
    "\n",
    "            train_mse.append(mean_squared_error(y_train,knn.predict(X_train)))\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            current_errors.append(mse)\n",
    "        var_error.append(np.var(current_errors))\n",
    "        mean_error.append(np.mean(current_errors))\n",
    "\n",
    "    var_error = np.array(var_error)\n",
    "    mean_error = np.array(mean_error)\n",
    "\n",
    "    plt.plot(neib_vect,mean_error)\n",
    "    plt.fill_between(neib_vect,mean_error-0.5*np.array(var_error),mean_error+0.5*var_error, alpha=0.5)\n",
    "    plt.title(\"Error on the validation set\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Number of neighbours k\")\n",
    "    plt.show()\n",
    "    best_hp = neib_vect[np.argmin(mean_error)]\n",
    "    print(f\"Minimal validation loss: {min(mean_error)} for n_neib = {best_hp}\")\n",
    "\n",
    "    best_knn = KNeighborsRegressor(n_neighbors=best_hp)\n",
    "    best_knn.fit(X_train,y_train)\n",
    "    plt.plot(y_val,best_knn.predict(X_val),'ob')\n",
    "    plt.plot(range(-3,4),range(-3,4))\n",
    "    plt.title(f\"Exemple of errors for n_neib = {best_hp}\")\n",
    "    plt.xlabel(\"Measured Yield Strength\")\n",
    "    plt.ylabel(f\"Estimated Yield Strength\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"==================================DECISION TREE==================================\\n\")\n",
    "\n",
    "    depth_vect = [1,2,3,4,5,6,7,8,9,10,15,20,25,30]\n",
    "    mean_error = []\n",
    "    var_error = []\n",
    "    train_mse = []\n",
    "    for depth in tqdm(depth_vect):\n",
    "        current_errors = []\n",
    "        for i in range(50):\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1415)\n",
    "            Xscaler = StandardScaler()\n",
    "            yscaler = StandardScaler()\n",
    "            X_train = Xscaler.fit_transform(X_train)\n",
    "            X_val = Xscaler.transform(X_val)\n",
    "            y_train = yscaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
    "            y_val = yscaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "            dt = DecisionTreeRegressor(max_depth=depth)\n",
    "\n",
    "            dt.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = dt.predict(X_val)\n",
    "\n",
    "            train_mse.append(mean_squared_error(y_train,dt.predict(X_train)))\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            current_errors.append(mse)\n",
    "        var_error.append(np.var(current_errors))\n",
    "        mean_error.append(np.mean(current_errors))\n",
    "\n",
    "    var_error = np.array(var_error)\n",
    "    mean_error = np.array(mean_error)\n",
    "\n",
    "    plt.plot(depth_vect,mean_error)\n",
    "    plt.fill_between(depth_vect,mean_error-0.5*np.array(var_error),mean_error+0.5*var_error,alpha=0.5,)\n",
    "    plt.title(\"Error on the validation set\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Max depth of the tree\")\n",
    "    plt.show()\n",
    "    best_hp = depth_vect[np.argmin(mean_error)]\n",
    "    print(f\"Minimal validation loss: {min(mean_error)} for max depth = {best_hp}\")\n",
    "\n",
    "    best_dt = DecisionTreeRegressor(max_depth=best_hp)\n",
    "    best_dt.fit(X_train,y_train)\n",
    "    plt.plot(y_val,best_dt.predict(X_val),'ob')\n",
    "    plt.plot(range(-3,4),range(-3,4))\n",
    "    plt.title(f\"Exemple of errors for max depth = {best_hp}\")\n",
    "    plt.xlabel(\"Measured Yield Strength\")\n",
    "    plt.ylabel(f\"Estimated Yield Strength\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"==================================RANDOM FOREST==================================\\n\")\n",
    "\n",
    "    depth_vect = [1,2,3,4,5,6,7,8,9,10,15,20]\n",
    "    mean_error = []\n",
    "    var_error = []\n",
    "    train_mse = []\n",
    "    for depth in tqdm(depth_vect):\n",
    "        current_errors = []\n",
    "        for i in range(10):\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1415)\n",
    "            Xscaler = StandardScaler()\n",
    "            yscaler = StandardScaler()\n",
    "            X_train = Xscaler.fit_transform(X_train)\n",
    "            X_val = Xscaler.transform(X_val)\n",
    "            y_train = yscaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
    "            y_val = yscaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "            rf = RandomForestRegressor(max_depth=depth)\n",
    "\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = rf.predict(X_val)\n",
    "\n",
    "            train_mse.append(mean_squared_error(y_train,rf.predict(X_train)))\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            current_errors.append(mse)\n",
    "        var_error.append(np.var(current_errors))\n",
    "        mean_error.append(np.mean(current_errors))\n",
    "\n",
    "    var_error = np.array(var_error)\n",
    "    mean_error = np.array(mean_error)\n",
    "\n",
    "    plt.plot(depth_vect,mean_error)\n",
    "    plt.fill_between(depth_vect,mean_error-0.5*np.array(var_error),mean_error+0.5*var_error,alpha=0.5,)\n",
    "    plt.title(\"Error on the validation set\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Max depth of the trees\")\n",
    "    plt.show()\n",
    "    best_hp = depth_vect[np.argmin(mean_error)]\n",
    "    print(f\"Minimal validation loss: {min(mean_error)} for max depth = {best_hp}\")\n",
    "\n",
    "    best_rf = RandomForestRegressor(max_depth=best_hp)\n",
    "    best_rf.fit(X_train,y_train)\n",
    "    plt.plot(y_val,best_rf.predict(X_val),'ob')\n",
    "    plt.plot(range(-3,4),range(-3,4))\n",
    "    plt.title(f\"Exemple of errors for max depth = {best_hp}\")\n",
    "    plt.xlabel(\"Measured Yield Strength\")\n",
    "    plt.ylabel(f\"Estimated Yield Strength\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"==================================SUPPORT VECTOR REGRESSION==================================\\n\")\n",
    "\n",
    "    mean_error = []\n",
    "    var_error = []\n",
    "    train_mse = []\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [5, 10, 15, 20,25,30],\n",
    "        'epsilon': [0.01,0.05, 0.1,0.15,0.25,0.5, 1],\n",
    "    }\n",
    "\n",
    "    for params in tqdm(ParameterGrid(param_grid)):\n",
    "        current_errors = []\n",
    "        for i in range(50):\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1415)\n",
    "            Xscaler = StandardScaler()\n",
    "            yscaler = StandardScaler()\n",
    "            X_train = Xscaler.fit_transform(X_train)\n",
    "            X_val = Xscaler.transform(X_val)\n",
    "            y_train = yscaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
    "            y_val = yscaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "            sv = SVR(C=params['C'],epsilon=params['epsilon'],kernel='rbf')\n",
    "\n",
    "            sv.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = sv.predict(X_val)\n",
    "\n",
    "            train_mse.append(mean_squared_error(y_train,sv.predict(X_train)))\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            current_errors.append(mse)\n",
    "        var_error.append(np.var(current_errors))\n",
    "        mean_error.append(np.mean(current_errors))\n",
    "\n",
    "    var_error = np.array(var_error)\n",
    "    mean_error = np.array(mean_error)\n",
    "\n",
    "    mean_error_matrix = mean_error.reshape(len(param_grid['C']),len(param_grid['epsilon']))\n",
    "\n",
    "    heatmap_data = pd.DataFrame(mean_error_matrix, index=param_grid['C'], columns=param_grid['epsilon'])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Mean Error'})\n",
    "    plt.title('Hyperparameter Tuning Heatmap (Mean Error)')\n",
    "    plt.xlabel('epsilon')\n",
    "    plt.ylabel('C')\n",
    "    plt.show()\n",
    "\n",
    "    min_error_index = np.unravel_index(np.argmin(mean_error_matrix, axis=None), mean_error_matrix.shape)\n",
    "    # Retrieve the best hyperparameters\n",
    "    best_C = param_grid['C'][min_error_index[0]]  # Row index corresponds to C values\n",
    "    best_epsilon = param_grid['epsilon'][min_error_index[1]]\n",
    "\n",
    "    print(f'Minimal validation loss: {np.min(mean_error_matrix,axis=None)} for C = {best_C} and epsilon = {best_epsilon}')\n",
    "\n",
    "    best_sv = SVR(C=best_C,epsilon=best_epsilon)\n",
    "    best_sv.fit(X_train,y_train)\n",
    "    plt.plot(y_val,best_sv.predict(X_val),'ob')\n",
    "    plt.plot(range(-3,4),range(-3,4))\n",
    "    plt.title(f\"Exemple of errors for C = {best_C} and eps = {best_epsilon}\")\n",
    "    plt.xlabel(\"Measured Yield Strength\")\n",
    "    plt.ylabel(f\"Estimated Yield Strength\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"==================================SUPPORT VECTOR REGRESSION SEMI SUPERVISED==================================\\n\")\n",
    "    if \"Charpy temperature (deg C)\" not in X.columns:\n",
    "        mean_error = []\n",
    "        var_error = []\n",
    "        train_mse = []\n",
    "\n",
    "        confidence_threshold = 0.9\n",
    "\n",
    "        param_grid = {\n",
    "            'C': [5, 10, 15, 20,25,30],\n",
    "            'epsilon': [0.01,0.05, 0.1,0.15,0.25,0.5, 1],\n",
    "        }\n",
    "        best_mse = 1000\n",
    "        for params in tqdm(ParameterGrid(param_grid)):\n",
    "            current_errors = []\n",
    "            for i in range(10):\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1415)\n",
    "                Xscaler = StandardScaler()\n",
    "                yscaler = StandardScaler()\n",
    "                X_train = Xscaler.fit_transform(X_train)\n",
    "                X_val = Xscaler.transform(X_val)\n",
    "                y_train = yscaler.fit_transform(y_train.reshape(-1,1)).reshape(-1)\n",
    "                y_val = yscaler.transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "                X_unlabeled_reg = Xscaler.transform(X_unlabeled)\n",
    "                sv = SVR(C=params['C'],epsilon=params['epsilon'],kernel='rbf')\n",
    "\n",
    "                for iteration in range(10):\n",
    "                    sv.fit(X_train, y_train)\n",
    "\n",
    "                    y_unlabeled_pred = sv.predict(X_unlabeled_reg)\n",
    "                    confidence_scores = predict_confidence(sv, X_unlabeled_reg)\n",
    "                    high_confidence_mask = confidence_scores > confidence_threshold\n",
    "                    if np.sum(high_confidence_mask) == 0:\n",
    "                        break\n",
    "                    \n",
    "                    X_train = np.vstack([X_train,X_unlabeled_reg[high_confidence_mask]])\n",
    "                    y_train = np.hstack([y_train,y_unlabeled_pred[high_confidence_mask]])\n",
    "                    X_unlabeled_reg = X_unlabeled_reg[~high_confidence_mask]\n",
    "\n",
    "                y_pred = sv.predict(X_val)\n",
    "\n",
    "                train_mse.append(mean_squared_error(y_train,sv.predict(X_train)))\n",
    "                mse = mean_squared_error(y_val, y_pred)\n",
    "                if mse< best_mse:\n",
    "                    best_model = sv\n",
    "                    best_mse = mse\n",
    "                    X_val_disp = X_val\n",
    "                    y_val_disp = y_val\n",
    "                current_errors.append(mse)\n",
    "            var_error.append(np.var(current_errors))\n",
    "            mean_error.append(np.mean(current_errors))\n",
    "\n",
    "        var_error = np.array(var_error)\n",
    "        mean_error = np.array(mean_error)\n",
    "\n",
    "        mean_error_matrix = mean_error.reshape(len(param_grid['C']),len(param_grid['epsilon']))\n",
    "\n",
    "        heatmap_data = pd.DataFrame(mean_error_matrix, index=param_grid['C'], columns=param_grid['epsilon'])\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Mean Error'})\n",
    "        plt.title('Hyperparameter Tuning Heatmap (Mean Error)')\n",
    "        plt.xlabel('epsilon')\n",
    "        plt.ylabel('C')\n",
    "        plt.show()\n",
    "\n",
    "        min_error_index = np.unravel_index(np.argmin(mean_error_matrix, axis=None), mean_error_matrix.shape)\n",
    "        # Retrieve the best hyperparameters\n",
    "        best_C = param_grid['C'][min_error_index[0]]  # Row index corresponds to C values\n",
    "        best_epsilon = param_grid['epsilon'][min_error_index[1]]\n",
    "\n",
    "        print(f'Minimal validation loss: {np.min(mean_error_matrix,axis=None)} for C = {best_C} and epsilon = {best_epsilon}')\n",
    "        plt.plot(y_val_disp,best_model.predict(X_val_disp),'ob')\n",
    "        plt.plot(range(-3,4),range(-3,4))\n",
    "        plt.title(f\"Exemple of errors for C = {best_C} and eps = {best_epsilon}\")\n",
    "        plt.xlabel(\"Measured Yield Strength\")\n",
    "        plt.ylabel(f\"Estimated Yield Strength\")\n",
    "        plt.show()\n",
    "\n",
    "    if 'Charpy temperature (deg C)' in X.columns:\n",
    "        X = X.drop('Charpy temperature (deg C)',axis=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
